from __future__ import annotations

import os
import time
from time import perf_counter
from datetime import datetime, timezone, timedelta
from typing import Optional, List, Dict, Any, Set, Tuple
from collections import deque
from psycopg2.extras import execute_values
import requests
from utils.logger import get_logger
from utils.model_key import normalise_model
from utils.db_schema import (
    resolve_source_id,
    resolve_source_field,
    get_connection,
    to_aware_utc,
    ensure_utc_session
)
from utils.usage_tracker import increment_api_usage

logger = get_logger(__name__)

# -----------------
# Constants / defaults
# -----------------
DEFAULT_Q = "a"  # minimal harmless primary query to satisfy Browse's rule


# ----------------------------------------------------------------------
# Helpers
# ----------------------------------------------------------------------
def _parse_iso_utc(ts: Optional[str]) -> Optional[datetime]:
    if not ts:
        return None
    try:
        ts_fixed = ts.replace("Z", "+00:00")
        dt = datetime.fromisoformat(ts_fixed)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        else:
            dt = dt.astimezone(timezone.utc)
        return dt
    except Exception:
        return None


def _secs_left(end_time: Optional[datetime]) -> Optional[int]:
    if not end_time:
        return None
    if end_time.tzinfo is None:
        end_aware = end_time.replace(tzinfo=timezone.utc)
    else:
        end_aware = end_time.astimezone(timezone.utc)
    now_aware = datetime.now(timezone.utc)
    delta = (end_aware - now_aware).total_seconds()
    return int(delta) if delta > 0 else 0


def _iso_z(dt: datetime) -> str:
    # ISO8601 with Z suffix
    return dt.replace(microsecond=0, tzinfo=timezone.utc).isoformat().replace("+00:00", "Z")


def is_configurable_item(raw: dict[str, Any]) -> bool:
    """
    Detect multi-variation / configurable-style listings so we can skip them entirely.
    """
    val = raw.get("isMultiVariationListing")
    if val is not None:
        if isinstance(val, (list, tuple)):
            if val:
                val = val[0]
        if isinstance(val, dict) and "__value__" in val:
            val = val.get("__value__")
        if isinstance(val, bool):
            if val:
                return True
        elif isinstance(val, str) and val.lower() == "true":
            return True

    group_type = raw.get("itemGroupType")
    if isinstance(group_type, str):
        gt = group_type.strip().upper()
        if gt in {"SELLER_DEFINED_VARIATIONS", "GROUP", "MULTI_SKU"}:
            return True

    if "variations" in raw or "variation" in raw:
        return True

    return False


def get_item_id(raw: dict[str, Any]) -> str:
    iid = raw.get("itemId") or raw.get("item_id") or raw.get("legacyItemId")
    return str(iid) if iid is not None else "unknown"


# ----------------------------------------------------------------------
# Base Adapter
# ----------------------------------------------------------------------
class EbayAdapterBase:
    DOMAIN: str = "ebay-base"

    CATEGORY_IDS: List[int] = []
    SELLER_USERNAME: Optional[str] = None
    FETCH_MODE: str = "category"
    SALE_TYPE: str | List[str] = "bin"
    RETRO_KEYWORDS: List[str] = []
    MODERN_KEYWORDS: List[str] = []

    # Tunables
    FLUSH_EVERY = 50
    FLUSH_SECONDS = 3
    MIN_TIME_BATCH = 10
    CATEGORY_PAUSE_SECONDS = 1.5

    # Politeness
    MAX_PAGES = 5
    MAX_LISTINGS = 200

    def __init__(self):
        self._source_name, self._source_id = self._resolve_source()
        self._batch_buffer: list[dict[str, Any]] = []
        self._ph_buffer: list[tuple[str, int, int]] = []
        self._last_flush = time.time()
        self._hist_api: deque[float] = deque(maxlen=500)
        self._hist_norm: deque[float] = deque(maxlen=500)
        self._hist_db: deque[float] = deque(maxlen=500)
        self._bench_n: int = 0

    # ------------------------------------------------------------------
    # Source resolution
    # ------------------------------------------------------------------
    def _resolve_source(self) -> tuple[str, Optional[int]]:
        try:
            sid = resolve_source_id(self.DOMAIN, use_domain=True)
            if sid is not None:
                sname = resolve_source_field(self.DOMAIN, "name", use_domain=True)
                logger.info(f"[{self.DOMAIN}] sources resolved by domain -> name='{sname}', id={sid}")
                return (str(sname) if sname else self.DOMAIN, int(sid))
        except Exception as e:
            logger.warning(f"[{self.DOMAIN}] domain lookup failed: {e}")

        try:
            sname = resolve_source_field(self.DOMAIN, "name", use_domain=False)
            if sname:
                sid = resolve_source_id(self.DOMAIN, use_domain=False)
                logger.info(
                    f"[{self.DOMAIN}] sources resolved by name -> name='{sname}', id={sid}"
                )
                return (str(sname), int(sid) if sid is not None else None)
        except Exception:
            pass

        for legacy_key in ("ebay-uk", "ebay"):
            try:
                sname = resolve_source_field(legacy_key, "name", use_domain=False)
                if sname:
                    sid = resolve_source_id(legacy_key, use_domain=False)
                    logger.info(
                        f"[{self.DOMAIN}] sources resolved via legacy key '{legacy_key}' -> name='{sname}', id={sid}"
                    )
                    return (str(sname), int(sid) if sid is not None else None)
            except Exception:
                continue

        logger.warning(f"[{self.DOMAIN}] sources row not found; using fallback '{self.DOMAIN}'")
        return self.DOMAIN, None

    # ------------------------------------------------------------------
    # Classification helpers
    # ------------------------------------------------------------------
    def categorize_title(self, title_lower: str) -> str:
        if any(k in title_lower for k in self.RETRO_KEYWORDS):
            return "retro"
        if any(k in title_lower for k in self.MODERN_KEYWORDS):
            return "modern"
        return "unknown"

    '''
    def _model_key_for(self, title: str) -> Optional[str]:
        try:
            mk = normalise_model(title)
        except Exception:
            return None
        if isinstance(mk, tuple):
            mk = mk[0]
        if not mk or (isinstance(mk, str) and not mk.strip()):
            return None
        return mk
    '''

    def _model_key_for(self, title: str) -> Optional[str]:
        return None

    def _is_relevant(self, row: dict[str, Any]) -> bool:
        return True

    # ------------------------------------------------------------------
    # DB flushing
    # ------------------------------------------------------------------
    def _maybe_flush(self):
        n_list = len(self._batch_buffer)
        n_hist = len(self._ph_buffer)
        n_total = n_list + n_hist
        due_by_size = n_list >= self.FLUSH_EVERY or n_hist >= (self.FLUSH_EVERY * 2)
        due_by_time = (time.time() - self._last_flush) >= self.FLUSH_SECONDS
        if (due_by_size or (due_by_time and n_total >= self.MIN_TIME_BATCH)) and n_total:
            self.flush_batch()

    def _filter_to_this_seller(self, collected: list[dict]) -> list[dict]:
        expected = getattr(self, "SELLER_USERNAME", None)
        if not expected:
            return collected
        expected_norm = expected.lower().strip()
        filtered: list[dict] = []
        for item in collected:
            seller_norm = (item.get("seller_username") or "").lower().strip()
            if seller_norm == expected_norm:
                filtered.append(item)
            else:
                logger.debug(
                    "[%s] dropped foreign seller '%s' (kept only '%s') for title=%r",
                    getattr(self, "DOMAIN", "?"),
                    seller_norm,
                    expected_norm,
                    item.get("title"),
                )
        return filtered

    def flush_batch(self):
        if not self._batch_buffer and not self._ph_buffer:
            return

        t0 = perf_counter()
        n_list = len(self._batch_buffer)
        n_hist = len(self._ph_buffer)

        try:
            if n_list:
                deduped: Dict[str, Dict[str, Any]] = {}
                for row in self._batch_buffer:
                    ext_id = row.get("external_id")
                    deduped[ext_id] = row
                safe_rows = list(deduped.values())

                bulk_upsert_auction_listings(safe_rows)
                self._batch_buffer.clear()

            if n_hist:
                try:
                    bulk_append_price_history(self._ph_buffer)
                except Exception as e:
                    logger.warning(f"[{self.DOMAIN}] bulk price_history failed: {e}")
                finally:
                    self._ph_buffer.clear()

        finally:
            dt = perf_counter() - t0
            self._last_flush = time.time()
            logger.info(
                f"[{self.DOMAIN}] bulk flush in {dt:.3f}s "
                f"(listings={n_list}, price_history={n_hist})"
            )

    # ------------------------------------------------------------------
    # eBay API helpers
    # ------------------------------------------------------------------
    def _build_headers(self, token: str) -> dict[str, str]:
        return {
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/json",
            "Accept": "application/json",
            "X-EBAY-C-MARKETPLACE-ID": "EBAY_GB",
        }

    def _fetch_category_items(
            self,
            token: str,
            category_id: int,
            sale_type: str | None = None,
            limit: int = 50,
    ) -> list[dict[str, Any]]:
        base = os.getenv("EBAY_API_BASE", "").rstrip("/")
        if not base:
            logger.error(f"[{self.DOMAIN}] EBAY_API_BASE missing in env")
            return []

        listing_filter = None
        if sale_type == "bin":
            listing_filter = "FIXED_PRICE"
        elif sale_type == "auction":
            listing_filter = "AUCTION"

        qs = [
            f"category_ids={category_id}",
            f"limit={limit}",
            "sort=endingSoon",
        ]
        if listing_filter:
            qs.append(f"filter=listingType:{listing_filter}")

        url = f"{base}/buy/browse/v1/item_summary/search?" + "&".join(qs)

        t_api_start = perf_counter()
        try:
            r = requests.get(url, headers=self._build_headers(token), timeout=10)
        except Exception as e:
            logger.warning(f"[{self.DOMAIN}] API request failed cat={category_id} ({sale_type}): {e}")
            return []

        self._hist_api.append(perf_counter() - t_api_start)
        if r.status_code != 200:
            logger.warning(
                f"[{self.DOMAIN}] API cat={category_id} ({sale_type}) status {r.status_code}: {r.text[:200]}"
            )
            return []

        increment_api_usage("ebay")

        try:
            payload = r.json()
        except Exception as e:
            logger.warning(f"[{self.DOMAIN}] bad JSON cat={category_id} ({sale_type}): {e}")
            return []

        items = payload.get("itemSummaries") or payload.get("item_summary") or []
        if not isinstance(items, list):
            logger.warning(
                f"[{self.DOMAIN}] unexpected payload for cat={category_id} ({sale_type})"
            )
            return []

        return items

    def _fetch_seller_items(
            self,
            token: str,
            seller_username: str,
            sale_type: str | None = None,
            limit: int = 50,
    ) -> list[dict[str, Any]]:
        base = os.getenv("EBAY_API_BASE", "").rstrip("/")
        if not base:
            logger.error(f"[{self.DOMAIN}] EBAY_API_BASE missing in env")
            return []

        buying_opt = None
        if sale_type == "bin":
            buying_opt = "FIXED_PRICE"
        elif sale_type == "auction":
            buying_opt = "AUCTION"

        window_days_options = [60, 14, 7]

        all_items: list[dict[str, Any]] = []
        seen_ids: set[str] = set()

        for days in window_days_options:
            start_iso = _iso_z(datetime.now(timezone.utc))
            end_iso = _iso_z(datetime.now(timezone.utc) + timedelta(days=days))

            filter_bits = [
                f"sellers:{{{seller_username}}}",
                f"itemEndDate:[{start_iso}..{end_iso}]",
                "itemLocationCountry:GB",
            ]
            if buying_opt:
                filter_bits.append(f"buyingOptions:{{{buying_opt}}}")
            filter_param = ",".join(filter_bits)

            offset = 0
            page_count = 0

            while True:
                qs = [
                    f"q={DEFAULT_Q}",
                    f"filter={filter_param}",
                    f"limit={limit}",
                    f"offset={offset}",
                    "sort=endingSoon",
                ]
                url = f"{base}/buy/browse/v1/item_summary/search?" + "&".join(qs)

                t_api_start = perf_counter()
                try:
                    r = requests.get(url, headers=self._build_headers(token), timeout=10)
                except Exception as e:
                    logger.warning(
                        f"[{self.DOMAIN}] API request failed seller={seller_username} ({sale_type}): {e}"
                    )
                    break

                self._hist_api.append(perf_counter() - t_api_start)

                if r.status_code != 200:
                    txt = (r.text or "").lower()
                    logger.warning(
                        f"[{self.DOMAIN}] API seller={seller_username} status {r.status_code}: {r.text[:200]}"
                    )
                    if r.status_code == 400 and ("too large" in txt or "too many" in txt):
                        break
                    else:
                        return all_items

                increment_api_usage("ebay")

                try:
                    payload = r.json()
                except Exception as e:
                    logger.warning(f"[{self.DOMAIN}] bad JSON seller={seller_username}: {e}")
                    break

                items = payload.get("itemSummaries") or payload.get("item_summary") or []
                if not isinstance(items, list) or not items:
                    break

                new_batch = []
                for it in items:
                    iid = it.get("itemId")
                    if not iid or iid in seen_ids:
                        continue
                    seen_ids.add(iid)
                    new_batch.append(it)

                if not new_batch:
                    break

                all_items.extend(new_batch)

                page_count += 1
                if page_count >= getattr(self, "MAX_PAGES", 10):
                    logger.info(
                        f"[{self.DOMAIN}] seller={seller_username} reached page cap ({page_count})"
                    )
                    break
                if len(all_items) >= getattr(self, "MAX_LISTINGS", 1000):
                    logger.info(
                        f"[{self.DOMAIN}] seller={seller_username} reached MAX_LISTINGS"
                    )
                    break

                offset += limit
                time.sleep(0.25)

            if all_items:
                break

        return all_items

    def _mark_404_listings_stale(self, external_ids: Set[str]) -> None:
        """
        For listings that returned 404 from getItem, mark them as stale in DB
        so they stop being treated as live targets.
        """
        if not external_ids:
            return

        from utils.db_schema import get_connection  # if not already imported at top

        conn = get_connection()
        ids_list = list(external_ids)

        try:
            with conn, conn.cursor() as cur:
                ensure_utc_session(cur)
                # Mark as stale and set end_time if it's NULL
                cur.execute(
                    """
                    UPDATE auction_listings
                    SET status   = 'stale',
                        end_time = COALESCE(end_time, (now() AT TIME ZONE 'utc'))
                    WHERE external_id = ANY(%s)
                    """,
                    (ids_list,),
                )
            logger.info(
                "[%s] marked %d listings as stale due to 404 from getItem",
                self.DOMAIN,
                len(ids_list),
            )
        except Exception as e:
            logger.warning(
                "[%s] failed to mark 404 listings stale (ids=%s): %s",
                self.DOMAIN,
                ids_list,
                e,
            )

    def _fetch_items_by_ids(
            self,
            token: str,
            external_ids: List[str],
            chunk_size: int = 20,
    ) -> List[Dict[str, Any]]:
        """
        Fetch full item details for a list of REST item IDs using the Browse getItem API.

        Used by the pph (price-per-hour) pipeline to refresh prices/bids for
        already-known listings without doing full discovery.
        """
        base = os.getenv("EBAY_API_BASE", "").rstrip("/")
        if not base:
            logger.error(f"[{self.DOMAIN}] EBAY_API_BASE missing in env")
            return []

        ids = [str(i) for i in external_ids if i]
        if not ids:
            return []

        all_items: List[Dict[str, Any]] = []
        seen_ids: Set[str] = set()
        not_found_ids: Set[str] = set()

        for eid in ids:
            if eid in seen_ids:
                continue
            seen_ids.add(eid)

            url = f"{base}/buy/browse/v1/item/{eid}"

            t_api_start = perf_counter()
            try:
                r = requests.get(url, headers=self._build_headers(token), timeout=10)
            except Exception as e:
                logger.warning(
                    "[%s] getItem request failed for ID %s: %s",
                    self.DOMAIN,
                    eid,
                    e,
                )
                continue

            self._hist_api.append(perf_counter() - t_api_start)

            # 404 == listing gone (ended/cancelled)
            if r.status_code == 404:
                logger.info("[%s] getItem 404 (ended?) for ID %s", self.DOMAIN, eid)
                not_found_ids.add(eid)
                continue

            if r.status_code != 200:
                logger.warning(
                    "[%s] getItem status %s for ID %s: %s",
                    self.DOMAIN,
                    r.status_code,
                    eid,
                    (r.text or "")[:200],
                )
                continue

            increment_api_usage("ebay")

            try:
                item = r.json()
            except Exception as e:
                logger.warning("[%s] bad JSON from getItem for ID %s: %s", self.DOMAIN, eid, e)
                continue

            all_items.append(item)
            time.sleep(0.05)

        # After we’re done, mark any 404’d listings as stale in DB
        if not_found_ids:
            self._mark_404_listings_stale(not_found_ids)

        return all_items

    def _normalize_item(self, raw: dict[str, Any], sale_type: str):
        if is_configurable_item(raw):
            logger.info(
                "[%s] skipping configurable/multi-variation listing itemId=%s title=%r",
                self.DOMAIN,
                get_item_id(raw),
                raw.get("title"),
            )
            return None

        item_id = raw.get("itemId")

        title = raw.get("title") or ""
        buying_opts = raw.get("buyingOptions") or []
        seller_info = raw.get("seller") or {}
        seller_username = seller_info.get("username")

        price_info = raw.get("price") or {}
        bid_info = raw.get("currentBidPrice") or {}

        price_value = price_info.get("value")
        bid_value = bid_info.get("value")

        web_url = raw.get("itemWebUrl") or raw.get("itemUrl") or ""
        end_time = _parse_iso_utc(raw.get("itemEndDate"))
        time_left_s = _secs_left(end_time)

        raw_bids = raw.get("bidCount")
        try:
            bids_count = int(raw_bids) if raw_bids is not None else 0
        except Exception:
            bids_count = 0

        if sale_type == "bin" and ("AUCTION" in buying_opts):
            return None
        if sale_type == "auction" and "AUCTION" not in buying_opts:
            return None

        title_lower = title.lower()
        model_key = self._model_key_for(title)

        def _to_int(val) -> Optional[int]:
            if val is None:
                return None
            try:
                return int(round(float(str(val))))
            except Exception:
                return None

        price_bid_current_int: Optional[int] = None
        price_current_int: Optional[int] = None

        if sale_type == "auction":
            price_bid_current_int = _to_int(bid_value)
            if price_bid_current_int is not None:
                price_current_int = price_bid_current_int
            else:
                price_current_int = _to_int(price_value)
        else:
            price_current_int = _to_int(price_value)

        row = {
            "source": self._source_name,
            "external_id": item_id,
            "title": title[:255],
            "price_current": price_current_int or 0,
            "price_bid_current": price_bid_current_int,
            "bids_count": bids_count,
            "end_time": end_time,
            "url": web_url[:1024],
            "sale_type": sale_type,
            "roi_estimate": None,
            "max_bid": None,
            "notes": '',
            "source_id": self._source_id,
            "model_key": model_key,
            "time_left_s": time_left_s,
            "status": "live",
            "seller_username": (seller_username or "").strip()[:255],
        }

        ph = (item_id, price_current_int or 0, bids_count)
        return row, ph

    # ------------------------------------------------------------------
    # Public entry: full scrape
    # ------------------------------------------------------------------
    def fetch_listings_api(self, ebay_token: str) -> None:
        sale_types = (
            self.SALE_TYPE if isinstance(self.SALE_TYPE, (list, tuple)) else [self.SALE_TYPE]
        )

        # SELLER MODE
        if self.FETCH_MODE == "seller":
            seller = self.SELLER_USERNAME
            if not seller:
                logger.warning(f"[{self.DOMAIN}] seller mode but no SELLER_USERNAME set")
                return

            for sale_type in sale_types:
                items = self._fetch_seller_items(ebay_token, seller, sale_type)
                if not items:
                    logger.info(f"[{self.DOMAIN}] seller {seller} {sale_type}: 0 items")
                    continue

                added = 0
                for raw in items:
                    norm = self._normalize_item(raw, sale_type)
                    if not norm:
                        continue
                    row, ph = norm

                    if row["seller_username"].lower().strip() != seller.lower().strip():
                        logger.debug(
                            "[%s] skipping foreign seller '%s' (wanted '%s') itemId=%s title=%r",
                            self.DOMAIN,
                            row["seller_username"],
                            seller,
                            row.get("external_id"),
                            row.get("title"),
                        )
                        continue

                    if not self._is_relevant(row):
                        continue

                    self._batch_buffer.append(row)
                    if row["price_current"]:
                        self._ph_buffer.append(ph)
                    added += 1
                    self._maybe_flush()

                logger.info(
                    f"[{self.DOMAIN}] seller {seller} {sale_type}: {added} listings"
                )

            self.flush_batch()
            return

        # CATEGORY MODE
        for cat_id in self.CATEGORY_IDS:
            for sale_type in sale_types:
                items = self._fetch_category_items(ebay_token, cat_id, sale_type)
                if not items:
                    logger.info(f"[{self.DOMAIN}] cat {cat_id} {sale_type}: 0 items")
                    time.sleep(self.CATEGORY_PAUSE_SECONDS)
                    continue

                added = 0
                for raw in items:
                    norm = self._normalize_item(raw, sale_type)
                    if not norm:
                        continue
                    row, ph = norm

                    if not self._is_relevant(row):
                        continue

                    self._batch_buffer.append(row)
                    if row["price_current"]:
                        self._ph_buffer.append(ph)
                    added += 1
                    self._maybe_flush()

                logger.info(f"[{self.DOMAIN}] cat {cat_id} {sale_type}: {added} listings")
                time.sleep(self.CATEGORY_PAUSE_SECONDS)

        self.flush_batch()

    # ------------------------------------------------------------------
    # Public entry: targeted price refresh (PPH)
    # ------------------------------------------------------------------
    def refresh_items_price(self, ebay_token: str, external_ids: List[str]) -> None:
        """
        Targeted price/bid refresh for already-known listings.

        Used by the pph (price-per-hour) pipeline:
          - ONLY hits listings we already know about (by external_id).
          - DOES NOT do discovery searches.
          - Reuses normalisation + bulk upsert + price history.
        """
        if not external_ids:
            logger.info("[%s] refresh_items_price called with empty ID list", self.DOMAIN)
            return

        items = self._fetch_items_by_ids(ebay_token, external_ids)
        if not items:
            logger.info(
                "[%s] refresh_items_price: 0 items returned for %d requested IDs",
                self.DOMAIN,
                len(external_ids),
            )
            return

        allowed_ids: Set[str] = {str(i) for i in external_ids if i}
        added = 0

        for raw in items:
            item_id = get_item_id(raw)
            if item_id not in allowed_ids:
                continue

            buying_opts = raw.get("buyingOptions") or []

            # Derive sale_type from live buying options if we can
            if "AUCTION" in buying_opts:
                sale_type = "auction"
            elif "FIXED_PRICE" in buying_opts:
                sale_type = "bin"
            else:
                # Fallback: use adapter's configured SALE_TYPE
                st = self.SALE_TYPE
                if isinstance(st, (list, tuple)):
                    if "auction" in st:
                        sale_type = "auction"
                    elif "bin" in st:
                        sale_type = "bin"
                    else:
                        sale_type = str(st[0])
                else:
                    sale_type = str(st)

            norm = self._normalize_item(raw, sale_type)
            if not norm:
                continue
            row, ph = norm

            if not self._is_relevant(row):
                continue

            self._batch_buffer.append(row)
            if row["price_current"]:
                self._ph_buffer.append(ph)
            added += 1
            self._maybe_flush()

        logger.info(
            "[%s] refresh_items_price: refreshed %d listings (requested=%d)",
            self.DOMAIN,
            added,
            len(external_ids),
        )

        self.flush_batch()


def ensure_utc_session(cur):
    try:
        cur.execute("SET TIME ZONE 'UTC'")
    except Exception:
        pass


def bulk_append_price_history(rows: list[tuple[str, int, int]]):
    """
    rows: (external_id, price, bids_count)
    """
    if not rows:
        return
    sql = """
        INSERT INTO auction_price_history (external_id, price, bids_count, recorded_at)
        VALUES %s
        ON CONFLICT DO NOTHING
    """
    conn = get_connection()  # Always get a live connection
    with conn, conn.cursor() as cur:
        ensure_utc_session(cur)
        cur.execute("SET LOCAL synchronous_commit TO OFF;")
        execute_values(
            cur,
            sql,
            rows,
            template="(%s, %s, %s, (now() AT TIME ZONE 'utc'))",
            page_size=500,
        )


def bulk_upsert_auction_listings(rows: list[dict]):
    """
    Bulk upsert of listings data from scrapers.
    Scraper semantics:
      - Every call represents "we have just seen these listings as ACTIVE on eBay".
      - This is the ONLY place that should bump last_seen_at.
    """
    if not rows:
        return

    # Consistent "seen alive" timestamp for this batch
    now = datetime.now(timezone.utc)

    # How long before a "live" listing is treated as stale/ended if we stop seeing it.
    # Default: 180 minutes (3 hours), override with GF_LISTING_STALE_MINUTES if needed.
    try:
        stale_minutes = int(os.getenv("GF_LISTING_STALE_MINUTES", "180"))
    except Exception:
        stale_minutes = 180
    stale_cutoff = now - timedelta(minutes=stale_minutes)

    for r in rows:
        # If end_time is missing → fabricate 1 day future end
        if not r.get("end_time"):
            r["end_time"] = now + timedelta(days=1)

    cols = [
        "source",
        "external_id",
        "title",
        "price_current",
        "bids_count",
        "end_time",
        "url",
        "sale_type",
        "roi_estimate",
        "max_bid",
        "notes",
        "source_id",
        "model_key",
        "time_left_s",
        "status",
        "last_seen_at",
    ]

    values: list[tuple] = []
    for r in rows:
        row_vals: list[Any] = []
        for c in cols:
            if c == "last_seen_at":
                # Always set "seen alive" to this batch timestamp
                row_vals.append(now)
            else:
                row_vals.append(r.get(c))
        values.append(tuple(row_vals))

    sql = f"""
        INSERT INTO auction_listings ({", ".join(cols)})
        VALUES %s
        ON CONFLICT (external_id) DO UPDATE
        SET title         = EXCLUDED.title,
            price_current = COALESCE(EXCLUDED.price_current, auction_listings.price_current),
            bids_count    = COALESCE(EXCLUDED.bids_count,    auction_listings.bids_count),
            end_time      = COALESCE(EXCLUDED.end_time,      auction_listings.end_time),
            url           = EXCLUDED.url,
            sale_type     = EXCLUDED.sale_type,
            roi_estimate  = EXCLUDED.roi_estimate,
            max_bid       = EXCLUDED.max_bid,
            notes         = EXCLUDED.notes,
            source_id     = EXCLUDED.source_id,
            model_key     = COALESCE(EXCLUDED.model_key,     auction_listings.model_key),
            time_left_s   = COALESCE(EXCLUDED.time_left_s,   auction_listings.time_left_s),
            status        = COALESCE(EXCLUDED.status,        auction_listings.status),
            last_seen_at  = EXCLUDED.last_seen_at
    """
    conn = get_connection()  # Always get a live connection
    with conn, conn.cursor() as cur:
        ensure_utc_session(cur)
        cur.execute("SET LOCAL synchronous_commit TO OFF;")

        # 1) Upsert this batch – everything in here is "seen alive" right now
        execute_values(cur, sql, values, page_size=250)

        # 2) Any listing that *used* to be live but hasn't been seen for a while
        #    is almost certainly ended (BIN hit, auction ended, cancelled, etc).
        #    We mark it as 'stale' so ROI / alerts can ignore it.
        cur.execute(
            """
            UPDATE auction_listings
            SET status = 'stale'
            WHERE status = 'live'
              AND last_seen_at < %s
            """,
            (stale_cutoff,),
        )
